{"metadata": {"kernelspec": {"display_name": "Python 2", "language": "python", "name": "python2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 2}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "version": "2.7.13"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Testing for forecasting models\n", "\n", "Testing a standard prediction model can be done using cross-validation. Usually we don't need to worry about ordering over observations so we can just go ahead and divide the data into K folds. However, we time-series data we need to take care to account for the chronological ordering of observations!\n", "\n", "We can account for the chronological ordering by making sure that we only use past observations when making predictions (this also makes it realistic in a production setting!).\n", "\n", "The remainder of this notebook will load some data and then show you how you can test a forecasting model!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import re\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "from statsmodels.api import tsa\n", "from dateutil.parser import parse\n", "\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.metrics import mean_absolute_error\n", "\n", "def parse_quarter(string):\n", "    \"\"\"\n", "    Converts a string from the format YYYYQN in datetime object at the end of quarter N.\n", "    \"\"\"\n", "    \n", "    # Note: you could also just retrieve the first four elements of the string\n", "    # and the last one... Regex is fun but often not necessary\n", "    year, qn = re.search(r'^(20[0-9][0-9])(Q[1-4])$', string).group(1, 2)\n", "    \n", "    # year and qn will be strings, pd.datetime expects integers.\n", "    year = int(year)\n", "    \n", "    date = None\n", "    \n", "    if qn=='Q1':\n", "        date = pd.datetime(year, 3, 31)\n", "    elif qn=='Q2':\n", "        date = pd.datetime(year, 6, 30)\n", "    elif qn=='Q3':\n", "        date = pd.datetime(year, 9, 20)\n", "    else:\n", "        date = pd.datetime(year, 12, 31)\n", "        \n", "    return date\n", "\n", "\n", "alcohol_consumption = pd.read_csv('data/NZAlcoholConsumption.csv', \n", "                                  parse_dates=['DATE'], \n", "                                  date_parser=parse_quarter,\n", "                                  index_col='DATE')\n", "alcohol_consumption.sort_index(inplace=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["wine = alcohol_consumption.TotalWine\n", "wine_diff = wine.diff(4).dropna()\n", "\n", "time_series = wine_diff"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Forecasting\n", "\n", "Until now we have tuned the model to *fit* the time series. \n", "This makes us able to find a description of the data, but doesn't give forecast.\n", "\n", "**Note**: forecasting with time series data is **tricky** and usually basic methods do not really provide very good results (especially on realistic data). ARMA models are nice because they are simple but do not expect fantastic performances. (On the other hand, predicting the future is hard! -- who would have thought).\n", "\n", "* Separate the time series into a training set and a test set formed of the last 8 points. \n", "* Fit an AR model on the training data and try to find the optimal lag using the `BIC` criterion\n", "* fit the model, predict and show the prediction on the original time series. Did it do a good job? \n", "* compute the MAE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# divide data into training and testing:\n", "train = time_series[:-8]\n", "test = time_series[-8:]\n", "\n", "# fit our AR model using the tsa module! \n", "ar = tsa.AR(train)\n", "\n", "# choose the optimal lag using BIC (this is a model selection criterion, dont worry if you dont know what it is :))\n", "optlag = ar.select_order(10, ic='bic', method='mle')\n", "\n", "# now we fit our model with the chosen optimal lag:\n", "arfit = ar.fit(maxlag=optlag)\n", "\n", "# we make predictions on unseen data:\n", "prediction = arfit.predict(end=len(time_series))[-len(test):]\n", "\n", "# produce some figures:\n", "plt.figure(figsize=(8, 6))\n", "plt.plot(time_series.values, '-o', label=\"original data\")\n", "plt.plot(prediction, '--o', label='prediction')\n", "\n", "plt.legend(fontsize=12)\n", "\n", "print('Out of sample MAE = {0:.3f}'.format(mean_absolute_error(test, prediction)))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cross-Validation for Time Series Forecasting (demo)\n", "\n", "Recall that it is important not to use future observations during a forecast! Below we will define a function to take a certain number of observations (ordered chronologically) as training data and use the remainder as testing data. We will also visualize their predictions :) \n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Forecasting_crossValidation( time_series, training_size ):\n", "    \"\"\"\n", "    Given a pd.Series we train a autoregressive model. \n", "    The number of training observation is specified by the variable training_size\n", "    \"\"\"\n", "    train = time_series[:training_size]\n", "    test = time_series[training_size:]\n", "    ar = tsa.AR(train)\n", "    ar_result = ar.fit(maxlag=4)\n", "    prediction = ar_result.predict(end=len(time_series))[-len(test):]\n", "    \n", "    # compute the MAE:\n", "    mae = mean_absolute_error(time_series.values[training_size:], prediction)\n", "    print('Mean absolute error: ' + str(mae))\n", "    \n", "    # plot results as well:\n", "    plt.plot(time_series.values, '-o', label='true')\n", "    plt.plot(range(training_size, len(time_series)), prediction, \n", "         '-o', label='out of sample prediction')\n", "    plt.legend();\n", "    \n", "    return prediction\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Forecasting_crossValidation( time_series, 43)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Excercise:\n", " - compute the average mean absolute error (MAE) for all possible forecasts (using a minimum of 20 training points). Hint: you may need to change your function to _return_ the MAE, and then loop over all different sizes of training sets :) \n", " \n", " \n", "#### Note:\n", "We may notice that the performance AR models isn't very good (at least visually it seems the models do a poor job of predicting future observations. There are many reasons this may be the case, one of them is that the data may be _non-stationary_! Indeed, if you observe a plot of the time-series data, we see there are large jumps present from around the 25th measurement onwards (the variance increases). "]}]}